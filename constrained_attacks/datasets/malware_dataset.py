import re
from typing import List

import numpy as np
from sklearn.model_selection import train_test_split

from constrained_attacks.constraints.constraints import (
    Constraints,
    get_constraints_from_metadata,
)
from constrained_attacks.constraints.relation_constraint import (
    Constant,
    Count,
    Feature,
    Log,
    ManySum,
)
from constrained_attacks.datasets.core import (
    MLC_DATA_PATH,
    DownloadFileDataset,
)
from constrained_attacks.datasets.processing import (
    get_numeric_categorical_preprocessor,
)


class MalwareDataset(DownloadFileDataset):
    def __init__(self):
        data_path = f"{MLC_DATA_PATH}/malware/malware.pkl"
        data_url = (
            "https://uniluxembourg-my.sharepoint.com/:u:/g/personal/"
            "thibault_simonetto_uni_lu/"
            "EUB3Dca0RhFEu8R8oYdHecIBOhJUA2WZbF3Js3zIjLZ2Sg?download=1"
        )
        metadata_path = f"{MLC_DATA_PATH}/malware/malware_metadata.csv"
        metadata_url = (
            "https://uniluxembourg-my.sharepoint.com/:x:/g/personal/"
            "thibault_simonetto_uni_lu/"
            "Ef-mGuyoTZtBqnUe6JA9HK4BiqyxHrVYgO7ssJ0Qe6T_AQ?download=1"
        )
        date = None
        date_format = None
        targets = ["malicious"]
        super().__init__(
            targets,
            data_path,
            data_url,
            metadata_path,
            metadata_url,
            date,
            date_format,
        )

    def get_preprocessor(self):
        return get_numeric_categorical_preprocessor(
            numeric_features=self.get_x_y()[0].columns,
            categorical_features=[],
        )

    def get_relation_constraints(self):
        x, _ = self.get_x_y()
        features = x.columns.to_list()

        def regex_features(
            features_l: List[str], pattern: str
        ) -> List[Feature]:
            p = re.compile(pattern)
            out = list(filter(lambda el: p.match(el), features_l))
            return [Feature(o) for o in out]

        features_g1 = regex_features(features, "^pesection_\d+_name$")
        g1_right = [f == Constant(832) for f in features_g1]
        g1 = Feature("header_NumberOfSections") == Count(
            g1_right, inverse=True
        )

        g2 = Feature("header_FileAlignment") <= Feature(
            "header_SectionAlignment"
        )

        g3 = Log(Feature("header_FileAlignment")) / Log(Constant(2.0))
        g3 = g3 % Constant(1.0)
        g3 = g3 == Constant(0.0)

        g4 = ManySum(regex_features(features, "^imp_.*$")) <= Feature(
            "api_import_nb"
        )

        g5 = ManySum(regex_features(features, "^.*\.dll$")) <= Feature(
            "dll_import_nb"
        )

        g6 = ManySum(regex_features(features, "^freq_byte_\d+$")) == Constant(
            1
        )

        freq = regex_features(features, "^freq_byte_\d+$")
        log_freq = [
            (f * (Log(f, safe_value=Constant(0.0)) / Log(Constant(2.0))))
            for f in freq
        ]
        g7 = Feature("generic_fileEntropy") == (
            Constant(0) - ManySum(log_freq)
        )

        return [g1, g2, g3, g4, g5, g6, g7]

    def get_constraints(self) -> Constraints:
        metadata = self.get_metadata()
        col_filter = self.get_x_y_t()[0].columns.to_list()
        return get_constraints_from_metadata(
            metadata, self.get_relation_constraints(), col_filter
        )

    def _get_splits(self):
        x, y = self.get_x_y()

        merged = np.arange(len(x))
        i_train, i_test = merged[:12308], merged[12308:]
        i_val, i_test = train_test_split(
            i_test, test_size=0.5, random_state=42
        )
        return {"train": i_train, "val": i_val, "test": i_test}
